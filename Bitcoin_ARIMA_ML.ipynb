{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"In this notebook we are trying to forecast the Weighted price from the Bitcoin dataset(). We will use ARIMA, Auto ARIMA and Recurrent Neural Networks(RNN) methods to forecast the Bitcoin Price Data from January 2012 to August 2019(Kaggle Bitcoin DataSet).\n\nThe following articles/blogs were referred in making this notebook:\n1.  https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/\n2.  https://machinelearningmastery.com/time-series-data-stationary-python/\n3.  https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\n4.  https://www.statisticshowto.datasciencecentral.com/unit-root/ \n5.  https://people.duke.edu/~rnau/411arim3.htm\n6.  https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pip install statsmodels --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install  pmdarima --upgrade\nimport pmdarima as pm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1 = pd.read_csv(\"../input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2019-08-12.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting timestamp to datetime object\ndf_1.Timestamp = pd.to_datetime(df_1.Timestamp, unit='s')\n\n# Resampling to daily frequency\ndf_1.index = df_1.Timestamp\ndf_1 = df_1.resample('D').mean()\n\n# Resampling to weekly frequency\ndf_week_1 = df_1.resample('W').mean()\n\n# Resampling to monthly frequency\ndf_month_1 = df_1.resample('M').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the NAN values from the dataset\ndf = df_week_1.Weighted_Price.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Overview of the data\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(df[200:])\nplt.xlabel(\"Date time\")\nplt.ylabel(\"Bitcoin Weighted Price\")\nplt.title(\" Bitcoin Price vs Time \")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Training and Test set \ntrain = df[200:349]   # we are not taking into account the initial data before 2016-01  \ntest = df[349:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(train,label='train')\nplt.plot(test,label='test')\nplt.legend()\nplt.title(\"Train and Test set data\")\nplt.xlabel(\"Datetime\")\nplt.ylabel(\"Bitcoin weighted price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Performing the Augemented Dickey Fuller test on our data to check for stationarity in the data\nprint(\"Dickeyâ€“Fuller test: p=%f\" % adfuller(train)[1])\nprint('Dickey-Fuller test ADF Statistic: %f' % adfuller(train)[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the p value (p=0.676423) is greater the 0.05 so we can reject the null hypothesis(The null hypothesis is that the time series is stationary.) and can say the series is not stationary.\nThe ADF statistic is -1.193412 which is quite negative indicating that we can strongly reject the null hypothesis. "},{"metadata":{},"cell_type":"markdown","source":"Our time series data is not stationary we will have to difference the series to make it stationary and determine the order of differencing that is required to make the series stationary. The order of differencing d, is inferred from the AutoCorrelation Function plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original Series\nfig, axes = plt.subplots(4, 2, sharex=True, figsize=(20,20))\naxes[0, 0].plot(train.values); axes[0, 0].set_title('Original Series')\nplot_acf(train.values, ax=axes[0, 1])\n\n# 1st Differencing\naxes[1, 0].plot(train.diff().values); axes[1, 0].set_title('1st Order Differencing')\nplot_acf(train.diff().dropna().values, ax=axes[1, 1])\n\n# 2nd Differencing\naxes[2, 0].plot(train.diff().diff().values); axes[2, 0].set_title('2nd Order Differencing')\nplot_acf(train.diff().diff().dropna().values, ax=axes[2, 1])\n\n# 3rd Differencing\naxes[3, 0].plot(train.diff().diff().diff().values); axes[3, 0].set_title('3rd Order Differencing')\nplot_acf(train.diff().diff().diff().dropna().values, ax=axes[3, 1])\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AutoCorrelation Function for 2nd order differencing becomes negative very quickly hinting that the series might be over differenced. So we can set the order of differencing d to 1 even though the series may not look very stationary.\n\nThe next step is to determine p which is the order of Auto Regressive(AR) terms required and q which is the order of Moving Average(MA) terms required.\n\nThe order of the AR terms p can be determind by looking at the Partial AutoCorrelation Plot (PACF) of the 1st differenced series."},{"metadata":{"trusted":true},"cell_type":"code","source":"# PACF plot of 1st differenced series\n#plt.rcParams.update({'figure.figsize':(6,3), 'figure.dpi':240})\n\nfig, axes = plt.subplots(1, 2, sharex=True, figsize=(6,3))\naxes[0].plot(train.diff().values); axes[0].set_title('1st Differencing')\naxes[1].set(ylim=(0,1.2))\n#axes[1].set(xlim=(0,300))\nplot_pacf(train.diff().dropna().values, ax=axes[1])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first two PACF lags are above the significance limit so we take p =2. This will also take care of the fact that the series might be slighlty under differenced.\n\nNext we want to determine the order of q by looking at the ACF plot of the 1st differenced series. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# ACF plot of 1st differenced series\n\n\nfig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))\naxes[0].plot(train.diff().values); axes[0].set_title('1st Differencing')\naxes[1].set(ylim=(0,1.2))\n#axes[1].set(xlim=(0,300))\nplot_acf(train.diff().dropna().values, ax=axes[1])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first two lags lie above the significance level. Since our series might be slightly under differenced so we set the value of q=1.\n\nSo our parameters for the ARIMA model are (1,2,1). We will now implement this model with the ARIMA package in statsmodels."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel = ARIMA(train.values, order=(1,2,1))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Actual vs Fitted\nmodel_fit.plot_predict(dynamic=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecast\nfc, se, conf = model_fit.forecast(50, alpha=0.05)  # 95% conf\n\n# Make as pandas series\nfc_series = pd.Series(fc, index=test.index)\nlower_series = pd.Series(conf[:, 0], index=test.index)\nupper_series = pd.Series(conf[:, 1], index=test.index)\n\n# Plot\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(train, label='training')\nplt.plot(test, label='actual')\nplt.plot(fc_series, label='forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will do the modeling of the time series data with Auto ARIMA"},{"metadata":{},"cell_type":"markdown","source":"# Auto ARIMA"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_auto = pm.auto_arima(train,start_p=1, start_q=1,\n                      test='adf',# use to determine the d \n                      max_p=3, max_q=3,\n                      m=1,              # frequency of series\n                      d=None,           # let model determine 'd'\n                      seasonal=False,   # No Seasonality\n                      start_P=0, \n                      D=0, \n                      trace=True,\n                      error_action='ignore',  \n                      suppress_warnings=True, \n                      stepwise=True)\n\nprint(model_auto.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The orders of the model are d=0,p=1,q=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_auto.plot_diagnostics(figsize=(7,7))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fc, conf = model_auto.predict(n_periods=50, return_conf_int=True, alpha=0.05)  # 95% conf\n\n# Make as pandas series\nfc_series    = pd.Series(fc, index=test.index)\nlower_series = pd.Series(conf[:, 0], index=test.index)\nupper_series = pd.Series(conf[:, 1], index=test.index)\n\n# Plot\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(train, label='training')\nplt.plot(test, label='actual')\nplt.plot(fc_series, label='forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals for Auto ARIMA')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now we will look for seasonality in the data series. "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scipy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.signal import find_peaks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding peaks in the time series data\npeaks, _ = find_peaks(train, height=0,distance=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"peaks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(train)\nplt.plot(train[peaks], \"x\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ignoring the first 10 peaks\npeak=peaks[10:]\nprint(np.diff(peak).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So lets take the seasonality to be 6. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(train,label='original')\nplt.plot(train.diff(6),label='seasonal differencing')\nplt.legend(loc='best', fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will decompose the series into the seasonality, trend and residues using the seasonal_decompose package from statsmodels. The methods used for the decomposition will be additive and multiplicative."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(train, model='additive')\n\nfig, (ax1,ax2,ax3,ax4) = plt.subplots(4,1, figsize=(20,19))\n#plt.title(\"Seasonal Decomposition\")\nresult.observed.plot(ax=ax1).set_ylabel('Original Series')\nresult.trend.plot(ax=ax2).set_ylabel('Trend')\nresult.seasonal.plot(ax=ax3).set_ylabel('Seasonal')\nresult.resid.plot(ax=ax4).set_ylabel('Residue')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_mu = seasonal_decompose(train, model='multiplicative')\n\nfig, (ax1,ax2,ax3,ax4) = plt.subplots(4,1, figsize=(20,19))\n#plt.title(\"Seasonal Decomposition Multiplicative\")\nresult_mu.observed.plot(ax=ax1).set_ylabel('Original Series')\nresult_mu.trend.plot(ax=ax2).set_ylabel('Trend')\nresult_mu.seasonal.plot(ax=ax3).set_ylabel('Seasonal')\nresult_mu.resid.plot(ax=ax4).set_ylabel('Residue')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can see from the seasonal decomposition plot that there is annual seasonality. \n\nNow we will use the Auto ARIMA model with seasonality"},{"metadata":{"trusted":true},"cell_type":"code","source":"sarima_model = pm.auto_arima(train,start_p=2, start_q=2,\n                      test='adf',# use to determine the d \n                      max_p=3, max_q=3,\n                      m=6,              # frequency of series\n                      d=None,           # let model determine 'd'\n                      seasonal=True,   # No Seasonality\n                      start_P=2, \n                      D=None, \n                      trace=True,\n                      error_action='ignore',  \n                      suppress_warnings=True, \n                      stepwise=True)\n\nprint(sarima_model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Seasonal Auto Arima also finds a seasonality of 6 which matches with what we found using the peak finding technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"sarima_model.plot_diagnostics(figsize=(7,7))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecast\nfc, conf = sarima_model.predict(n_periods=50, return_conf_int=True, alpha=0.05)  # 95% conf\n\n# Make as pandas series\nfc_series    = pd.Series(fc, index=test.index)\nlower_series = pd.Series(conf[:, 0], index=test.index)\nupper_series = pd.Series(conf[:, 1], index=test.index)\n\n# Plot\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(train, label='training')\nplt.plot(test, label='actual')\nplt.plot(fc_series, label='forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Approach"},{"metadata":{},"cell_type":"markdown","source":"\nNow we will extend the approch to using Recurrent Neural Networks(RNNs). \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transformations on data and pre processing for LSTM RNNs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sliding Window Transformation\nf=4 #Size of input layer \nx= np.zeros((len(df)-f,f))\nfor i in range(len(df)-f):\n    x[i,:]=df[i:i+f]\ny=df[f:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalisation\nx/=df.max()\n\ny/=df.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split into train and test set\nn=(350 -f) \n\nx_train= x[200:n,:]\nx_test = x[n:,:]\n\ny_train= y[200:n]\ny_test = y[n:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_train,label='train')\nplt.plot(y_test,label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n\n# define model\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_dim=f))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nmodel.fit(x_train, y_train, epochs=len(y_train), verbose=0)\n#Multilayer Perceptron model or MLP","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\nyhat = model.predict(x_train[-1,:].reshape((1,f)), verbose=0)\nx_new = x_train[-1,:]\nyhat_list = [yhat]\nfor i in range(y_test.size):\n    _l = list(x_new[(1-f):])\n    _l.append(yhat)\n    x_new = np.array(_l)\n    yhat = model.predict(x_new.reshape((1,f)), verbose=0)\n    yhat_list.append(float(yhat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"float(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(yhat_list,label='predict')\nplt.plot(y_test.values,label='actual test set')\n#plt.plot(y_train,label='train_data')\n#plt.plot(df2,label='complete')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing the output from the RNN model to Seasonal Auto Arima"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot\n\nfc_ml_series    = pd.Series(yhat_list, index=test.index)\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(train/df.max(), label='training')\nplt.plot(test/df.max(), label='actual')\nplt.plot(fc_series/df.max(), label='forecast ARIMA')\nplt.plot(fc_ml_series, label='forecast RNN')\nplt.fill_between(lower_series.index, lower_series/df.max(), upper_series/df.max(), \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals for ARIMA AND RNN')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}